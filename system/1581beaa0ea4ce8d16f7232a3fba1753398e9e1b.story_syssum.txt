After all, even though today's artificial intelligence can beat humans within narrow domains (such as chess or trivia games), machine brains are still extremely rudimentary in general intelligence.
We know that evolutionary processes can produce human-level general intelligence, because they have already done so at least once in Earth's history.
One likely path would be to continue studying the general properties of the human brain to decipher the computational structures it uses to generate intelligent behavior.
The next stop from human level intelligence, just a short distance farther along the tracks, is machine superintelligence.
If there will eventually be an "intelligence explosion," how exactly can we set up the initial conditions so as to achieve an outcome that is survivable and beneficial to existing persons?
A superintelligence wouldn't even need to start with a physical embodiment to be catastrophically dangerous.
It should be solvable in principle, but in practice it may not be solved in time for when the solution is needed.
If we could solve the technical problem of constructing a motivation system that we can load with some terminal goal of our choosing, a further question remains: Which goal would we give the superintelligent A.I.?
It would be a grave mistake to think we have reached our moral apogee, and thus lock our present-day ethics into such powerful machines.
What would happen if machines surpassed human intellect as well?
After all, even though today's artificial intelligence can beat humans within narrow domains (such as chess or trivia games), machine brains are still extremely rudimentary in general intelligence.
By 2050 we may, according to a recent survey of leading artificial intelligence researchers, have a 50/50 chance of achieving human-level machine intelligence (defined here as "one that can carry out most human professions at least as well as a typical human").
Regardless of when and how we get there, the consequences of reaching human-level machine intelligence are profound, because human-level machine intelligence is not the final destination.
The next stop from human level intelligence, just a short distance farther along the tracks, is machine superintelligence.
In "Superintelligence: Paths, Dangers, Strategies," I focus on the dynamics of an intelligence explosion; what will happen if and when we gain the ability to create machine superintelligence?
A superintelligence wouldn't even need to start with a physical embodiment to be catastrophically dangerous.
The difficulty is compounded by the need to get it right on the first try.
Remember HAL from "2001: A Space Odyssey"?
Much would hinge on that choice.
Some may think we have already arrived upon full moral enlightenment, but is is far more likely that we still have blind spots.
Our wisdom must precede our technology, and that which we value in life must be carefully articulated—or rather, it must be pointed to with the right mathematics—if it is to be the seed from which our intelligent creations grow.
What would happen if machines surpassed human intellect as well?
By 2050 we may, according to a recent survey of leading artificial intelligence researchers, have a 50/50 chance of achieving human-level machine intelligence (defined here as "one that can carry out most human professions at least as well as a typical human").
The next stop from human level intelligence, just a short distance farther along the tracks, is machine superintelligence.
In "Superintelligence: Paths, Dangers, Strategies," I focus on the dynamics of an intelligence explosion; what will happen if and when we gain the ability to create machine superintelligence?
A superintelligence wouldn't even need to start with a physical embodiment to be catastrophically dangerous.
The difficulty is compounded by the need to get it right on the first try.
Some may think we have already arrived upon full moral enlightenment, but is is far more likely that we still have blind spots.
In this sense, we have philosophy with a deadline.